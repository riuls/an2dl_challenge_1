{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WHPhlAnlljh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04daa578-85fc-4515-ff8d-e1da3748a58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/Deep Learning Challenge\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Deep Learning Challenge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from data_preparation import load_data, delete_outliers\n",
        "import numpy as np\n",
        "from visualization import plot_history\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "xCvC6R5RlrD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 25\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ],
      "metadata": {
        "id": "U27hz-RalrGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data exploration and preparation utilities"
      ],
      "metadata": {
        "id": "nTsO-1lqSGym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(folder=\"public_data.npz\", resolution=96, head_only=False):\n",
        "    images = []\n",
        "\n",
        "    loaded = np.load(folder, allow_pickle=True)\n",
        "\n",
        "    # Iterate through files in the specified folder\n",
        "    for i, img in enumerate(loaded['data']):\n",
        "        # Normalize image pixel values to a float range [0, 1]\n",
        "        img = (img / 255).astype(np.float32)\n",
        "\n",
        "        # Convert image from BGR to RGB\n",
        "        #img = img[...,::-1]\n",
        "\n",
        "        # Make the image dataset squared\n",
        "        dim = min(img.shape[:-1])\n",
        "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2, (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
        "\n",
        "        #Image resizing\n",
        "        img = tfkl.Resizing(resolution, resolution)(img)\n",
        "\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "        if (head_only and i == 9):\n",
        "           break\n",
        "\n",
        "    labels = loaded['labels']\n",
        "    loaded.close()\n",
        "\n",
        "    if (head_only):\n",
        "       labels = labels[:10]\n",
        "\n",
        "    y = LabelEncoder().fit_transform(labels)\n",
        "\n",
        "    return np.array(images), y\n",
        "\n",
        "def display_random_images(X, y, num_img=10):\n",
        "  # Create subplots for displaying items\n",
        "  fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
        "  for i in range(num_img):\n",
        "      image = random.randint(0, X.shape[0] - 1)\n",
        "\n",
        "      ax = axes[i%2, i%num_img//2]\n",
        "      ax.imshow(np.clip(X[image], 0, 255))  # Display clipped item images\n",
        "      ax.text(0.5, -0.1, str(image) + ' ' + str(y[image]), size=12, ha=\"center\", transform=ax.transAxes)\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "#Outliers are deleted simply by comparison with known outlier samples\n",
        "def delete_outliers(X, y):\n",
        "  shrek = 137\n",
        "  trololo = 5143\n",
        "\n",
        "  new_X = []\n",
        "  new_y = []\n",
        "\n",
        "  num_outliers = 0\n",
        "\n",
        "  for i, sample in enumerate(X):\n",
        "    if (not (np.array_equal(sample, X[shrek]) or np.array_equal(sample, X[trololo]))):\n",
        "      new_X.append(sample)\n",
        "      new_y.append(y[i])\n",
        "    else:\n",
        "      num_outliers += 1\n",
        "\n",
        "  return np.array(new_X), np.array(new_y), num_outliers\n",
        "\n",
        "#Accuracy, recall and precision are plotted\n",
        "def plot_history(history, name, patience=20):\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "    plt.plot(history['val_accuracy'], label='Val Accuracy', alpha=.8, color='#ff7f0e')\n",
        "    plt.plot(history['val_precision'], label='Val Precision', alpha=.8, color='red')\n",
        "    plt.plot(history['val_recall'], label='Val Recall', alpha=.8, color='blue')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(name + ' Accuracy')\n",
        "    plt.grid(alpha=.3)\n",
        "\n",
        "    plt.text(0.5, -0.1, 'Train accuracy: ' + str(round(history['accuracy'][-patience-1], 4)) + ', Val accuracy: ' + str(round(np.max(history['val_accuracy']), 4)), horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    plt.figure(figsize=(11,5))\n",
        "    plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "    plt.plot(history['val_loss'], label='Val', alpha=.8, color='#ff7f0e')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('Categorical Crossentropy')\n",
        "    plt.grid(alpha=.3)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ajzPa3OIlrIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading end preparation"
      ],
      "metadata": {
        "id": "wRX5CI7MSNst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_data('public_data.npz')\n",
        "X, y, num_outliers = delete_outliers(X, y)"
      ],
      "metadata": {
        "id": "5l74ZbmulrLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "X_train_0, X_val, y_train_0, y_val = train_test_split(X, y, test_size=500, stratify=y, random_state=seed)\n",
        "\n",
        "print(X_train_0.shape, y_train_0.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "dQZ2uDNJlrNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70437aa2-d734-419c-a928-437bd93fba56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4504, 96, 96, 3) (4504,)\n",
            "(500, 96, 96, 3) (500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Two different augmentations are implemented, augmenting the whole dataset 2 times and augmenting the minority class one time\n",
        "def augment_dataset(X, y):\n",
        "  augment1 = tf.keras.Sequential([\n",
        "      tfkl.RandomFlip(),\n",
        "      tfkl.RandomTranslation(height_factor = (-0.2,0.2), width_factor = (-0.2,0.2), fill_mode = 'reflect'),\n",
        "      tfkl.RandomZoom(0.3, fill_mode = 'reflect'),\n",
        "      tfkl.RandomBrightness(0.1, value_range=(0,1)),\n",
        "  ])\n",
        "\n",
        "  augment2 = tf.keras.Sequential([\n",
        "      tfkl.RandomFlip(),\n",
        "      tfkl.RandomZoom(0.4, fill_mode = 'reflect'),\n",
        "      tfkl.RandomBrightness(0.1, value_range=(0,1)),\n",
        "      tfkl.RandomRotation((-1,1), fill_mode = 'reflect'),\n",
        "  ])\n",
        "\n",
        "  new_X_train_1 = augment2(X[y == 1])\n",
        "  augmented_X_train_2 = augment2(X)\n",
        "  augmented_X_train_1 = augment1(augmented_X_train_2)\n",
        "\n",
        "  X = np.append(X, augmented_X_train_2, axis = 0)\n",
        "  X = np.append(X, augmented_X_train_1, axis = 0)\n",
        "  X = np.append(X, new_X_train_1, axis = 0)\n",
        "\n",
        "  y_tmp = np.append(y, y)\n",
        "  y = np.append(y_tmp, y)\n",
        "  for k in range(new_X_train_1.shape[0]):\n",
        "      y = np.append(y, 1)\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "Mj4QnhPqlrPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = augment_dataset(X_train_0, y_train_0)\n",
        "\n",
        "#Dataset is shuffle after augmentation\n",
        "indices = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "# Define key model parameters\n",
        "input_shape = X_train.shape[1:]\n",
        "batch_size = 16\n",
        "epochs = 400\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(np.unique(y_train, return_counts=True))"
      ],
      "metadata": {
        "id": "qLWKZnxhlrR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_random_images(X_train, y_train)"
      ],
      "metadata": {
        "id": "mI_T2_fQs8V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model building and validation"
      ],
      "metadata": {
        "id": "uUlWOhKdSSY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specifies a single block in the architecture\n",
        "def get_block(x, neurons, batch_normalization=False, dropout=0):\n",
        "    x = tfkl.Dense(neurons)(x)\n",
        "    if (batch_normalization):\n",
        "      x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Activation('relu')(x)\n",
        "    x = tfkl.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "#Function to easily select models, in the parameters you can specify neurons and layers, dropout levels, and dropout after the pretrained model.\n",
        "def build_model(input_shape, learning_rate=1e-4, neurons=[], base_dropout=0, dropouts=[], batch_normalization=False, name=\"model\", weight_decay=3e-5):\n",
        "  mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        "  )\n",
        "\n",
        "  mobile.trainable = False\n",
        "  inputs = tfk.Input(shape=input_shape)\n",
        "  # Connect MobileNetV2 to the input\n",
        "  x = mobile(inputs)\n",
        "  x = tfkl.Dropout(base_dropout)(x)\n",
        "\n",
        "  for i, n in enumerate(neurons):\n",
        "      x = get_block(x, n, batch_normalization=batch_normalization, dropout=dropouts[i])\n",
        "\n",
        "  # Add a Dense layer with 1 unit and sigmoid activation as the classifier\n",
        "  outputs = tfkl.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  # Create a Model connecting input and output\n",
        "  model = tfk.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "\n",
        "  # Compile the model with Binary Cross-Entropy loss and AdamW optimizer\n",
        "  model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay), metrics=['accuracy', tfk.metrics.Precision(name=\"precision\"), tfk.metrics.Recall(name=\"recall\")])\n",
        "\n",
        "  # Display model summary\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "-oliHEELlrX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    build_model(input_shape, learning_rate=1e-4, neurons=[64, 16], dropouts=[1/7, 1/7], base_dropout=1/5, batch_normalization=True, weight_decay=3e-5),\n",
        "]\n",
        "\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "class_weight = {\n",
        "    0: weights[0],\n",
        "    1: weights[1]\n",
        "}\n",
        "\n",
        "histories = []\n",
        "\n",
        "patience = 30\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  histories.append(model.fit(\n",
        "      x = tfk.applications.mobilenet.preprocess_input(X_train * 255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "      y = y_train,\n",
        "      #class_weight = class_weight,\n",
        "      batch_size = 16,\n",
        "      epochs = 400,\n",
        "      validation_data = (tfk.applications.mobilenet.preprocess_input(X_val * 255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "      callbacks = [\n",
        "          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience, restore_best_weights=True),\n",
        "          tfk.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 15, min_lr = 5e-6)\n",
        "      ]\n",
        "  ).history)"
      ],
      "metadata": {
        "id": "g8PvVuy7mH3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "for history in histories:\n",
        "  #Training and validation accuracy, precision and recall are plotted, together with area under the curve\n",
        "  predictions = models[i].predict(tfk.applications.mobilenet.preprocess_input(X_val * 255))\n",
        "  predictions = (predictions > 0.5).astype(\"int32\")\n",
        "  fpr, tpr, thresholds = roc_curve(y_val, predictions)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print('AUC: ' + str(roc_auc))\n",
        "  print('Best Epoch: ' + str(len(history['val_accuracy']) - patience))\n",
        "  plot_history(history, patience=30, name=\"\")"
      ],
      "metadata": {
        "id": "DQex_SGMmsGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[0]\n",
        "model.save(\"TransferLearningModel\")\n",
        "del model"
      ],
      "metadata": {
        "id": "vbvY6j8fmH8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine Tuning"
      ],
      "metadata": {
        "id": "cRWIc4wfdzdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Builds a fine tuned model, with the given number of unfrozen layers\n",
        "def fine_tune(layers, learning_rate):\n",
        "  model = tf.keras.models.load_model(\"TransferLearningModel\")\n",
        "\n",
        "  layers = len(model.get_layer('mobilenetv2_1.00_96').layers) - layers\n",
        "\n",
        "  model.get_layer('mobilenetv2_1.00_96').trainable = True\n",
        "\n",
        "  # Freeze first N layers, e.g., until the 133rd one\n",
        "  for i, layer in enumerate(model.get_layer('mobilenetv2_1.00_96').layers[:layers]):\n",
        "    layer.trainable=False\n",
        "\n",
        "  for layer in model.get_layer('mobilenetv2_1.00_96').layers[layers:]:\n",
        "    print(layer)\n",
        "\n",
        "  model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate, weight_decay=weight_decay), metrics=['accuracy', Precision(name=\"precision\"), Recall(name=\"recall\")])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "lVLwlmQbUchJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histories = []\n",
        "\n",
        "\n",
        "models = [\n",
        "    fine_tune(12, 1e-5),\n",
        "]\n",
        "\n",
        "patience = 20\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "class_weights = {\n",
        "    0: weights[0],\n",
        "    1: weights[1]\n",
        "}\n",
        "\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  histories.append(\n",
        "      model.fit(\n",
        "        x = tfk.applications.mobilenet.preprocess_input(X_train * 255),\n",
        "        y = y_train,\n",
        "        class_weight=class_weights,\n",
        "        batch_size = batch_size,\n",
        "        epochs = 200,\n",
        "        validation_data = (tfk.applications.mobilenet.preprocess_input(X_val * 255), y_val),\n",
        "        callbacks = [\n",
        "          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience, restore_best_weights=True, start_from_epoch=5),\n",
        "          #tfk.callbacks.ModelCheckpoint('modelB' + str(i) + '_weights_epoch_{epoch:02d}.h5', save_weights_only=True, save_freq=5 * steps_per_epoch),\n",
        "          tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-7, mode='max')\n",
        "        ]\n",
        "  ).history)"
      ],
      "metadata": {
        "id": "TsmY6cBdzTno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, history in enumerate(histories):\n",
        "  #Training and validation accuracy, precision and recall are plotted, together with area under the curve\n",
        "  predictions = models[i].predict(tfk.applications.mobilenet.preprocess_input(X_val * 255))\n",
        "  predictions = (predictions > 0.5).astype(\"int32\")\n",
        "  fpr, tpr, thresholds = roc_curve(y_val, predictions)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(roc_auc)\n",
        "  plot_history(history, models[i].name, patience=patience)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erGzo711x0zD",
        "outputId": "6bbb36ec-4b7d-4e6c-af87-ac8a6297e2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy 0.8120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models[0].save(\"Submission\")"
      ],
      "metadata": {
        "id": "uWbGf8SS7Mhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}