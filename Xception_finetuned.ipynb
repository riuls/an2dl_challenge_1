{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4WHPhlAnlljh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac93edcc-dcb8-4bdf-dcac-a1937edeb5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Deep Learning Challenge\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/Deep Learning Challenge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from data_preparation import load_data, delete_outliers\n",
        "import numpy as np\n",
        "from visualization import plot_history\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "xCvC6R5RlrD9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 25\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ],
      "metadata": {
        "id": "U27hz-RalrGS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(folder=\"public_data.npz\", resolution=96, head_only=False):\n",
        "    images = []\n",
        "\n",
        "    loaded = np.load(folder, allow_pickle=True)\n",
        "\n",
        "    # Iterate through files in the specified folder\n",
        "    for i, img in enumerate(loaded['data']):\n",
        "        # Normalize image pixel values to a float range [0, 1]\n",
        "        img = (img / 255).astype(np.float32)\n",
        "\n",
        "        # Convert image from BGR to RGB\n",
        "        #img = img[...,::-1]\n",
        "\n",
        "        # Make the image dataset squared\n",
        "        dim = min(img.shape[:-1])\n",
        "        img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2, (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
        "\n",
        "        # Resize the image to 224x224 pixels\n",
        "        #img = tfkl.Resizing(224, 224)(img)\n",
        "        img = tfkl.Resizing(resolution, resolution)(img)\n",
        "\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "        if (head_only and i == 9):\n",
        "           break\n",
        "\n",
        "    labels = loaded['labels']\n",
        "    loaded.close()\n",
        "\n",
        "    if (head_only):\n",
        "       labels = labels[:10]\n",
        "\n",
        "    y = LabelEncoder().fit_transform(labels)\n",
        "    #y = tfk.utils.to_categorical(y, 2)\n",
        "\n",
        "    return np.array(images), y\n",
        "\n",
        "def display_random_images(X, y, num_img=10):\n",
        "  # Create subplots for displaying items\n",
        "  fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))\n",
        "  for i in range(num_img):\n",
        "      image = random.randint(0, X.shape[0] - 1)\n",
        "\n",
        "      ax = axes[i%2, i%num_img//2]\n",
        "      ax.imshow(np.clip(X[image], 0, 255))  # Display clipped item images\n",
        "      ax.text(0.5, -0.1, str(image) + ' ' + str(y[image]), size=12, ha=\"center\", transform=ax.transAxes)\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def delete_outliers(X, y):\n",
        "  shrek = 137\n",
        "  trololo = 5143\n",
        "\n",
        "  new_X = []\n",
        "  new_y = []\n",
        "\n",
        "  num_outliers = 0\n",
        "\n",
        "  for i, sample in enumerate(X):\n",
        "    if (not (np.array_equal(sample, X[shrek]) or np.array_equal(sample, X[trololo]))):\n",
        "      new_X.append(sample)\n",
        "      new_y.append(y[i])\n",
        "    else:\n",
        "      num_outliers += 1\n",
        "\n",
        "  return np.array(new_X), np.array(new_y), num_outliers\n",
        "\n",
        "\n",
        "def plot_history(history, name, patience=20):\n",
        "    # Plot the re-trained MobileNetV2 training history\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "    plt.plot(history['val_loss'], label='Re-trained', alpha=.8, color='#ff7f0e')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title('Categorical Crossentropy')\n",
        "    plt.grid(alpha=.3)\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
        "    plt.plot(history['val_accuracy'], label='Val Accuracy', alpha=.8, color='#ff7f0e')\n",
        "    plt.plot(history['val_precision'], label='Val Precision', alpha=.8, color='red')\n",
        "    plt.plot(history['val_recall'], label='Val Recall', alpha=.8, color='blue')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(name + ' Accuracy')\n",
        "    plt.grid(alpha=.3)\n",
        "\n",
        "    plt.text(0.5, -0.1, 'Train accuracy: ' + str(round(history['accuracy'][-patience-1], 4)) + ', Val accuracy: ' + str(round(np.max(history['val_accuracy']), 4)), horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ajzPa3OIlrIe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_data('public_data.npz')\n",
        "X, y, num_outliers = delete_outliers(X, y)"
      ],
      "metadata": {
        "id": "5l74ZbmulrLF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train_val and test sets\n",
        "X_train_0, X_val, y_train_0, y_val = train_test_split(X, y, test_size=500, stratify=y, random_state=seed)\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=500, stratify=np.argmax(y_train_val,axis=1))\n",
        "\n",
        "print(X_train_0.shape, y_train_0.shape)\n",
        "print(X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "dQZ2uDNJlrNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c3a1ea-1bdc-4ea3-ebaf-e2b0e708fe82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4504, 96, 96, 3) (4504,)\n",
            "(500, 96, 96, 3) (500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_dataset1(X, y):\n",
        "  augment1 = tf.keras.Sequential([\n",
        "      tfkl.RandomFlip(),\n",
        "      tfkl.RandomTranslation(height_factor = (-0.2,0.2), width_factor = (-0.2,0.2), fill_mode = 'reflect'),\n",
        "      tfkl.RandomZoom(0.3, fill_mode = 'reflect'),\n",
        "      tfkl.RandomBrightness(0.1, value_range=(0,1)),\n",
        "  ])\n",
        "\n",
        "  augment2 = tf.keras.Sequential([\n",
        "      tfkl.RandomFlip(),\n",
        "      tfkl.RandomZoom(0.4, fill_mode = 'reflect'),\n",
        "      tfkl.RandomBrightness(0.1, value_range=(0,1)),\n",
        "      tfkl.RandomRotation((-1,1), fill_mode = 'reflect'),\n",
        "  ])\n",
        "\n",
        "  new_X_train_1 = augment2(X[np.where((y[:, 0] == 0) & (y[:, 1] == 1))])\n",
        "  augmented_X_train_2 = augment2(X)\n",
        "  augmented_X_train_1 = augment1(augmented_X_train_2)\n",
        "\n",
        "  X = np.append(X, augmented_X_train_2, axis = 0)\n",
        "  X = np.append(X, augmented_X_train_1, axis = 0)\n",
        "  X = np.append(X, new_X_train_1, axis = 0)\n",
        "\n",
        "  y_tmp = np.append(y, y, axis = 0)\n",
        "  y = np.append(y_tmp, y, axis = 0)\n",
        "  for k in range(new_X_train_1.shape[0]):\n",
        "      y = np.append(y, [[0,1]], axis = 0)\n",
        "  return X, y\n",
        "\n",
        "def augment_dataset(X, y):\n",
        "  augmentation = tf.keras.Sequential([\n",
        "      tfkl.RandomRotation((-1, 1), fill_mode = 'reflect'),\n",
        "      tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      tfkl.RandomBrightness((-0.3, 0.1), value_range=(0,1)),\n",
        "  ])\n",
        "\n",
        "  X_tmp = X[y == 1]\n",
        "  X = np.append(X, augmentation(X), axis=0)\n",
        "  X = np.append(X, augmentation(X_tmp), axis=0)\n",
        "\n",
        "  y = np.append(y, y)\n",
        "  y = np.append(y, np.ones(len(X_tmp)))\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "Mj4QnhPqlrPj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = augment_dataset(X_train_0, y_train_0)\n",
        "\n",
        "#Shuffle the dataset\n",
        "indices = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X_train = X_train[indices]\n",
        "y_train = y_train[indices]\n",
        "\n",
        "# Define key model parameters\n",
        "input_shape = X_train.shape[1:]  # Input shape for the model  # Output shape for the model\n",
        "batch_size = 16                # Batch size for training, always a power of 2!!\n",
        "epochs = 400\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(np.unique(y_train, return_counts=True))"
      ],
      "metadata": {
        "id": "qLWKZnxhlrR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c491e01a-967f-4a56-c386-ac0e2e49b0b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10721, 96, 96, 3)\n",
            "(10721,)\n",
            "(array([0., 1.]), array([5582, 5139]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_random_images(X_train, y_train)"
      ],
      "metadata": {
        "id": "mI_T2_fQs8V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_block(x, neurons, batch_normalization=False, dropout=0):\n",
        "    x = tfkl.Dense(neurons)(x)\n",
        "    if (batch_normalization):\n",
        "      x = tfkl.BatchNormalization()(x)\n",
        "    x = tfkl.Activation('relu')(x)\n",
        "    x = tfkl.Dropout(dropout)(x)\n",
        "    return x\n",
        "\n",
        "def build_model(input_shape, learning_rate=1e-4, neurons=[], base_dropout=0, dropout=0, batch_normalization=False, name=\"model\", weight_decay=3e-5):\n",
        "  mobile = tfk.applications.MobileNetV2(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg',\n",
        "  )\n",
        "\n",
        "  mobile.trainable = False\n",
        "  inputs = tfk.Input(shape=input_shape)\n",
        "  # Connect MobileNetV2 to the input\n",
        "  x = mobile(inputs)\n",
        "  x = tfkl.Dropout(base_dropout)(x)\n",
        "\n",
        "  for n in neurons:\n",
        "      x = get_block(x, n, batch_normalization=batch_normalization, dropout=dropout)\n",
        "\n",
        "  # Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "  outputs = tfkl.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  # Create a Model connecting input and output\n",
        "  model = tfk.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "\n",
        "  # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
        "  model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay), metrics=['accuracy', tfk.metrics.Precision(name=\"precision\"), tfk.metrics.Recall(name=\"recall\")])\n",
        "\n",
        "  # Display model summary\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "-oliHEELlrX0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    build_model(input_shape, learning_rate=8e-4, neurons=[256], dropout=0.3, base_dropout=0.1, batch_normalization=True),\n",
        "    #build_model(input_shape)\n",
        "]\n",
        "\n",
        "weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "class_weight = {\n",
        "    0: weights[0],\n",
        "    1: weights[1]\n",
        "}\n",
        "\n",
        "histories = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  histories.append(model.fit(\n",
        "      x = tfk.applications.mobilenet.preprocess_input(X_train * 255), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "      y = y_train,\n",
        "      class_weight = class_weight,\n",
        "      batch_size = 16,\n",
        "      epochs = 400,\n",
        "      validation_data = (tfk.applications.mobilenet.preprocess_input(X_val * 255), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "      callbacks = [\n",
        "          tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n",
        "          tfk.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 15, min_lr = 1e-5)\n",
        "      ]\n",
        "  ).history)"
      ],
      "metadata": {
        "id": "g8PvVuy7mH3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826a74f5-7686-4a1e-94c2-18810fd5af6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 2048)              20861480  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21387305 (81.59 MB)\n",
            "Trainable params: 525313 (2.00 MB)\n",
            "Non-trainable params: 20861992 (79.58 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/400\n",
            "671/671 [==============================] - 32s 25ms/step - loss: 0.5843 - accuracy: 0.7004 - precision: 0.6791 - recall: 0.7110 - val_loss: 0.4372 - val_accuracy: 0.7960 - val_precision: 0.7651 - val_recall: 0.6684 - lr: 8.0000e-04\n",
            "Epoch 2/400\n",
            "671/671 [==============================] - 16s 23ms/step - loss: 0.5176 - accuracy: 0.7423 - precision: 0.7237 - recall: 0.7478 - val_loss: 0.4356 - val_accuracy: 0.8020 - val_precision: 0.7241 - val_recall: 0.7737 - lr: 8.0000e-04\n",
            "Epoch 3/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4950 - accuracy: 0.7539 - precision: 0.7333 - recall: 0.7649 - val_loss: 0.4522 - val_accuracy: 0.7640 - val_precision: 0.6429 - val_recall: 0.8526 - lr: 8.0000e-04\n",
            "Epoch 4/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4831 - accuracy: 0.7628 - precision: 0.7435 - recall: 0.7712 - val_loss: 0.4174 - val_accuracy: 0.8160 - val_precision: 0.7849 - val_recall: 0.7105 - lr: 8.0000e-04\n",
            "Epoch 5/400\n",
            "671/671 [==============================] - 14s 22ms/step - loss: 0.4712 - accuracy: 0.7676 - precision: 0.7513 - recall: 0.7700 - val_loss: 0.4339 - val_accuracy: 0.7820 - val_precision: 0.6709 - val_recall: 0.8368 - lr: 8.0000e-04\n",
            "Epoch 6/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4566 - accuracy: 0.7772 - precision: 0.7583 - recall: 0.7856 - val_loss: 0.4474 - val_accuracy: 0.7840 - val_precision: 0.7204 - val_recall: 0.7053 - lr: 8.0000e-04\n",
            "Epoch 7/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4424 - accuracy: 0.7858 - precision: 0.7666 - recall: 0.7953 - val_loss: 0.3928 - val_accuracy: 0.8200 - val_precision: 0.7747 - val_recall: 0.7421 - lr: 8.0000e-04\n",
            "Epoch 8/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4375 - accuracy: 0.7910 - precision: 0.7721 - recall: 0.8002 - val_loss: 0.4206 - val_accuracy: 0.8160 - val_precision: 0.7988 - val_recall: 0.6895 - lr: 8.0000e-04\n",
            "Epoch 9/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4305 - accuracy: 0.7932 - precision: 0.7751 - recall: 0.8009 - val_loss: 0.4568 - val_accuracy: 0.7760 - val_precision: 0.6612 - val_recall: 0.8421 - lr: 8.0000e-04\n",
            "Epoch 10/400\n",
            "671/671 [==============================] - 16s 24ms/step - loss: 0.4178 - accuracy: 0.8035 - precision: 0.7846 - recall: 0.8132 - val_loss: 0.4066 - val_accuracy: 0.8240 - val_precision: 0.7500 - val_recall: 0.8053 - lr: 8.0000e-04\n",
            "Epoch 11/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.4138 - accuracy: 0.8012 - precision: 0.7837 - recall: 0.8085 - val_loss: 0.4071 - val_accuracy: 0.8200 - val_precision: 0.7604 - val_recall: 0.7684 - lr: 8.0000e-04\n",
            "Epoch 12/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.4115 - accuracy: 0.8073 - precision: 0.7934 - recall: 0.8085 - val_loss: 0.4106 - val_accuracy: 0.8180 - val_precision: 0.7391 - val_recall: 0.8053 - lr: 8.0000e-04\n",
            "Epoch 13/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3988 - accuracy: 0.8106 - precision: 0.7959 - recall: 0.8134 - val_loss: 0.4202 - val_accuracy: 0.8220 - val_precision: 0.7853 - val_recall: 0.7316 - lr: 8.0000e-04\n",
            "Epoch 14/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3823 - accuracy: 0.8201 - precision: 0.8043 - recall: 0.8255 - val_loss: 0.4052 - val_accuracy: 0.8140 - val_precision: 0.7299 - val_recall: 0.8105 - lr: 8.0000e-04\n",
            "Epoch 15/400\n",
            "671/671 [==============================] - 15s 23ms/step - loss: 0.3810 - accuracy: 0.8246 - precision: 0.8083 - recall: 0.8313 - val_loss: 0.4382 - val_accuracy: 0.8020 - val_precision: 0.7333 - val_recall: 0.7526 - lr: 8.0000e-04\n",
            "Epoch 16/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3804 - accuracy: 0.8235 - precision: 0.8058 - recall: 0.8325 - val_loss: 0.4428 - val_accuracy: 0.7980 - val_precision: 0.6878 - val_recall: 0.8579 - lr: 8.0000e-04\n",
            "Epoch 17/400\n",
            "671/671 [==============================] - 15s 23ms/step - loss: 0.3730 - accuracy: 0.8285 - precision: 0.8100 - recall: 0.8389 - val_loss: 0.3983 - val_accuracy: 0.8300 - val_precision: 0.7692 - val_recall: 0.7895 - lr: 8.0000e-04\n",
            "Epoch 18/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3622 - accuracy: 0.8322 - precision: 0.8163 - recall: 0.8387 - val_loss: 0.4129 - val_accuracy: 0.8100 - val_precision: 0.7778 - val_recall: 0.7000 - lr: 8.0000e-04\n",
            "Epoch 19/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3572 - accuracy: 0.8318 - precision: 0.8126 - recall: 0.8437 - val_loss: 0.4035 - val_accuracy: 0.8160 - val_precision: 0.7379 - val_recall: 0.8000 - lr: 8.0000e-04\n",
            "Epoch 20/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3608 - accuracy: 0.8354 - precision: 0.8216 - recall: 0.8387 - val_loss: 0.4458 - val_accuracy: 0.8080 - val_precision: 0.7176 - val_recall: 0.8158 - lr: 8.0000e-04\n",
            "Epoch 21/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3471 - accuracy: 0.8430 - precision: 0.8273 - recall: 0.8500 - val_loss: 0.3878 - val_accuracy: 0.8280 - val_precision: 0.7766 - val_recall: 0.7684 - lr: 8.0000e-04\n",
            "Epoch 22/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3371 - accuracy: 0.8484 - precision: 0.8329 - recall: 0.8554 - val_loss: 0.4132 - val_accuracy: 0.8200 - val_precision: 0.8049 - val_recall: 0.6947 - lr: 8.0000e-04\n",
            "Epoch 23/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3389 - accuracy: 0.8435 - precision: 0.8293 - recall: 0.8480 - val_loss: 0.4036 - val_accuracy: 0.8220 - val_precision: 0.7463 - val_recall: 0.8053 - lr: 8.0000e-04\n",
            "Epoch 24/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3323 - accuracy: 0.8512 - precision: 0.8354 - recall: 0.8589 - val_loss: 0.4119 - val_accuracy: 0.8060 - val_precision: 0.7385 - val_recall: 0.7579 - lr: 8.0000e-04\n",
            "Epoch 25/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3203 - accuracy: 0.8562 - precision: 0.8436 - recall: 0.8593 - val_loss: 0.4209 - val_accuracy: 0.8120 - val_precision: 0.7500 - val_recall: 0.7579 - lr: 8.0000e-04\n",
            "Epoch 26/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.3266 - accuracy: 0.8515 - precision: 0.8362 - recall: 0.8583 - val_loss: 0.4973 - val_accuracy: 0.7880 - val_precision: 0.6707 - val_recall: 0.8684 - lr: 8.0000e-04\n",
            "Epoch 27/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3135 - accuracy: 0.8596 - precision: 0.8443 - recall: 0.8671 - val_loss: 0.4569 - val_accuracy: 0.7960 - val_precision: 0.6913 - val_recall: 0.8368 - lr: 8.0000e-04\n",
            "Epoch 28/400\n",
            "671/671 [==============================] - 15s 23ms/step - loss: 0.3187 - accuracy: 0.8534 - precision: 0.8370 - recall: 0.8620 - val_loss: 0.4634 - val_accuracy: 0.7840 - val_precision: 0.6723 - val_recall: 0.8421 - lr: 8.0000e-04\n",
            "Epoch 29/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3071 - accuracy: 0.8650 - precision: 0.8514 - recall: 0.8704 - val_loss: 0.4164 - val_accuracy: 0.8060 - val_precision: 0.7409 - val_recall: 0.7526 - lr: 8.0000e-04\n",
            "Epoch 30/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.3050 - accuracy: 0.8629 - precision: 0.8482 - recall: 0.8696 - val_loss: 0.4500 - val_accuracy: 0.8060 - val_precision: 0.6996 - val_recall: 0.8579 - lr: 8.0000e-04\n",
            "Epoch 31/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.2995 - accuracy: 0.8637 - precision: 0.8514 - recall: 0.8671 - val_loss: 0.4630 - val_accuracy: 0.8020 - val_precision: 0.7040 - val_recall: 0.8263 - lr: 8.0000e-04\n",
            "Epoch 32/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.2922 - accuracy: 0.8718 - precision: 0.8565 - recall: 0.8801 - val_loss: 0.4510 - val_accuracy: 0.7960 - val_precision: 0.6982 - val_recall: 0.8158 - lr: 8.0000e-04\n",
            "Epoch 33/400\n",
            "671/671 [==============================] - 14s 22ms/step - loss: 0.2636 - accuracy: 0.8877 - precision: 0.8740 - recall: 0.8947 - val_loss: 0.4548 - val_accuracy: 0.8160 - val_precision: 0.7207 - val_recall: 0.8421 - lr: 4.0000e-04\n",
            "Epoch 34/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2556 - accuracy: 0.8892 - precision: 0.8765 - recall: 0.8949 - val_loss: 0.4449 - val_accuracy: 0.8040 - val_precision: 0.7371 - val_recall: 0.7526 - lr: 4.0000e-04\n",
            "Epoch 35/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2562 - accuracy: 0.8879 - precision: 0.8772 - recall: 0.8908 - val_loss: 0.4244 - val_accuracy: 0.8100 - val_precision: 0.7487 - val_recall: 0.7526 - lr: 4.0000e-04\n",
            "Epoch 36/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2584 - accuracy: 0.8893 - precision: 0.8770 - recall: 0.8945 - val_loss: 0.4467 - val_accuracy: 0.8160 - val_precision: 0.7579 - val_recall: 0.7579 - lr: 4.0000e-04\n",
            "Epoch 37/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2487 - accuracy: 0.8956 - precision: 0.8848 - recall: 0.8994 - val_loss: 0.4823 - val_accuracy: 0.7960 - val_precision: 0.7095 - val_recall: 0.7842 - lr: 4.0000e-04\n",
            "Epoch 38/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2450 - accuracy: 0.8917 - precision: 0.8789 - recall: 0.8978 - val_loss: 0.4523 - val_accuracy: 0.8160 - val_precision: 0.7426 - val_recall: 0.7895 - lr: 4.0000e-04\n",
            "Epoch 39/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2394 - accuracy: 0.8951 - precision: 0.8875 - recall: 0.8945 - val_loss: 0.4813 - val_accuracy: 0.8080 - val_precision: 0.7238 - val_recall: 0.8000 - lr: 4.0000e-04\n",
            "Epoch 40/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2387 - accuracy: 0.8986 - precision: 0.8868 - recall: 0.9039 - val_loss: 0.4411 - val_accuracy: 0.8040 - val_precision: 0.7300 - val_recall: 0.7684 - lr: 4.0000e-04\n",
            "Epoch 41/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2340 - accuracy: 0.8991 - precision: 0.8879 - recall: 0.9035 - val_loss: 0.4446 - val_accuracy: 0.8080 - val_precision: 0.7448 - val_recall: 0.7526 - lr: 4.0000e-04\n",
            "Epoch 42/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2336 - accuracy: 0.8974 - precision: 0.8871 - recall: 0.9006 - val_loss: 0.4566 - val_accuracy: 0.8020 - val_precision: 0.7661 - val_recall: 0.6895 - lr: 4.0000e-04\n",
            "Epoch 43/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2242 - accuracy: 0.9018 - precision: 0.8911 - recall: 0.9058 - val_loss: 0.4377 - val_accuracy: 0.8100 - val_precision: 0.7273 - val_recall: 0.8000 - lr: 4.0000e-04\n",
            "Epoch 44/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2293 - accuracy: 0.9026 - precision: 0.8925 - recall: 0.9060 - val_loss: 0.4820 - val_accuracy: 0.7980 - val_precision: 0.7089 - val_recall: 0.7947 - lr: 4.0000e-04\n",
            "Epoch 45/400\n",
            "671/671 [==============================] - 14s 21ms/step - loss: 0.2288 - accuracy: 0.9029 - precision: 0.8928 - recall: 0.9062 - val_loss: 0.4551 - val_accuracy: 0.8140 - val_precision: 0.7622 - val_recall: 0.7421 - lr: 4.0000e-04\n",
            "Epoch 46/400\n",
            "671/671 [==============================] - 14s 22ms/step - loss: 0.2261 - accuracy: 0.9018 - precision: 0.8929 - recall: 0.9035 - val_loss: 0.4537 - val_accuracy: 0.8000 - val_precision: 0.7273 - val_recall: 0.7579 - lr: 4.0000e-04\n",
            "Epoch 47/400\n",
            "671/671 [==============================] - 15s 22ms/step - loss: 0.2240 - accuracy: 0.9040 - precision: 0.8937 - recall: 0.9078 - val_loss: 0.4648 - val_accuracy: 0.8000 - val_precision: 0.6991 - val_recall: 0.8316 - lr: 4.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for history in histories:\n",
        "  plot_history(history, patience=30, name=\"\")"
      ],
      "metadata": {
        "id": "DQex_SGMmsGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_accuracy = model_1.evaluate(tfk.applications.mobilenet.preprocess_input(X_test),y_test,verbose=0)[-1]\n",
        "print('Test set accuracy %.4f' % test_accuracy)"
      ],
      "metadata": {
        "id": "S7E-lu3LmH5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2cb7c3-989a-44c7-b3f0-59487a0e8393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy 0.8220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.save('Xception_NO_INVERSION_nobatchnorm')\n",
        "del model_1"
      ],
      "metadata": {
        "id": "vbvY6j8fmH8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = tfk.models.load_model('Xception_NO_INVERSION')"
      ],
      "metadata": {
        "id": "TsmY6cBdzTno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.get_layer('xception').trainable = True\n",
        "ft_model.get_layer('xception').summary()"
      ],
      "metadata": {
        "id": "h2A_uYFguqVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze up to last N layers\n",
        "N = 7    # last block\n",
        "for i, layer in enumerate(ft_model.get_layer('xception').layers[:-N]):\n",
        "  layer.trainable=False"
      ],
      "metadata": {
        "id": "CliFxQGduqYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(ft_model.get_layer('xception').layers):\n",
        "  print(layer.name, layer.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yPceu-Cuqac",
        "outputId": "658b8b04-adfd-4615-bce4-cdfd3f5c0709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_1 False\n",
            "block1_conv1 False\n",
            "block1_conv1_bn False\n",
            "block1_conv1_act False\n",
            "block1_conv2 False\n",
            "block1_conv2_bn False\n",
            "block1_conv2_act False\n",
            "block2_sepconv1 False\n",
            "block2_sepconv1_bn False\n",
            "block2_sepconv2_act False\n",
            "block2_sepconv2 False\n",
            "block2_sepconv2_bn False\n",
            "conv2d False\n",
            "block2_pool False\n",
            "batch_normalization False\n",
            "add False\n",
            "block3_sepconv1_act False\n",
            "block3_sepconv1 False\n",
            "block3_sepconv1_bn False\n",
            "block3_sepconv2_act False\n",
            "block3_sepconv2 False\n",
            "block3_sepconv2_bn False\n",
            "conv2d_1 False\n",
            "block3_pool False\n",
            "batch_normalization_1 False\n",
            "add_1 False\n",
            "block4_sepconv1_act False\n",
            "block4_sepconv1 False\n",
            "block4_sepconv1_bn False\n",
            "block4_sepconv2_act False\n",
            "block4_sepconv2 False\n",
            "block4_sepconv2_bn False\n",
            "conv2d_2 False\n",
            "block4_pool False\n",
            "batch_normalization_2 False\n",
            "add_2 False\n",
            "block5_sepconv1_act False\n",
            "block5_sepconv1 False\n",
            "block5_sepconv1_bn False\n",
            "block5_sepconv2_act False\n",
            "block5_sepconv2 False\n",
            "block5_sepconv2_bn False\n",
            "block5_sepconv3_act False\n",
            "block5_sepconv3 False\n",
            "block5_sepconv3_bn False\n",
            "add_3 False\n",
            "block6_sepconv1_act False\n",
            "block6_sepconv1 False\n",
            "block6_sepconv1_bn False\n",
            "block6_sepconv2_act False\n",
            "block6_sepconv2 False\n",
            "block6_sepconv2_bn False\n",
            "block6_sepconv3_act False\n",
            "block6_sepconv3 False\n",
            "block6_sepconv3_bn False\n",
            "add_4 False\n",
            "block7_sepconv1_act False\n",
            "block7_sepconv1 False\n",
            "block7_sepconv1_bn False\n",
            "block7_sepconv2_act False\n",
            "block7_sepconv2 False\n",
            "block7_sepconv2_bn False\n",
            "block7_sepconv3_act False\n",
            "block7_sepconv3 False\n",
            "block7_sepconv3_bn False\n",
            "add_5 False\n",
            "block8_sepconv1_act False\n",
            "block8_sepconv1 False\n",
            "block8_sepconv1_bn False\n",
            "block8_sepconv2_act False\n",
            "block8_sepconv2 False\n",
            "block8_sepconv2_bn False\n",
            "block8_sepconv3_act False\n",
            "block8_sepconv3 False\n",
            "block8_sepconv3_bn False\n",
            "add_6 False\n",
            "block9_sepconv1_act False\n",
            "block9_sepconv1 False\n",
            "block9_sepconv1_bn False\n",
            "block9_sepconv2_act False\n",
            "block9_sepconv2 False\n",
            "block9_sepconv2_bn False\n",
            "block9_sepconv3_act False\n",
            "block9_sepconv3 False\n",
            "block9_sepconv3_bn False\n",
            "add_7 False\n",
            "block10_sepconv1_act False\n",
            "block10_sepconv1 False\n",
            "block10_sepconv1_bn False\n",
            "block10_sepconv2_act False\n",
            "block10_sepconv2 False\n",
            "block10_sepconv2_bn False\n",
            "block10_sepconv3_act False\n",
            "block10_sepconv3 False\n",
            "block10_sepconv3_bn False\n",
            "add_8 False\n",
            "block11_sepconv1_act False\n",
            "block11_sepconv1 False\n",
            "block11_sepconv1_bn False\n",
            "block11_sepconv2_act False\n",
            "block11_sepconv2 False\n",
            "block11_sepconv2_bn False\n",
            "block11_sepconv3_act False\n",
            "block11_sepconv3 False\n",
            "block11_sepconv3_bn False\n",
            "add_9 False\n",
            "block12_sepconv1_act False\n",
            "block12_sepconv1 False\n",
            "block12_sepconv1_bn False\n",
            "block12_sepconv2_act False\n",
            "block12_sepconv2 False\n",
            "block12_sepconv2_bn False\n",
            "block12_sepconv3_act False\n",
            "block12_sepconv3 False\n",
            "block12_sepconv3_bn False\n",
            "add_10 False\n",
            "block13_sepconv1_act False\n",
            "block13_sepconv1 False\n",
            "block13_sepconv1_bn False\n",
            "block13_sepconv2_act False\n",
            "block13_sepconv2 False\n",
            "block13_sepconv2_bn False\n",
            "conv2d_3 False\n",
            "block13_pool False\n",
            "batch_normalization_3 False\n",
            "add_11 False\n",
            "block14_sepconv1 True\n",
            "block14_sepconv1_bn True\n",
            "block14_sepconv1_act True\n",
            "block14_sepconv2 True\n",
            "block14_sepconv2_bn True\n",
            "block14_sepconv2_act True\n",
            "global_average_pooling2d True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics='accuracy')"
      ],
      "metadata": {
        "id": "z6BTmLxHuqdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = ft_model.fit(\n",
        "    x = tfk.applications.xception.preprocess_input(X_train), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    y = y_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 400,\n",
        "    validation_data = (tfk.applications.xception.preprocess_input(X_val), y_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
        "    callbacks = [\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.9, patience = 8, min_lr = 2e-6)\n",
        "    ]\n",
        ").history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4ZMNtgEuqgh",
        "outputId": "74a6f2fd-63a5-4e7b-81b7-483e71015d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "846/846 [==============================] - 45s 31ms/step - loss: 0.5213 - accuracy: 0.7436 - val_loss: 0.4153 - val_accuracy: 0.8140 - lr: 0.0010\n",
            "Epoch 2/400\n",
            "846/846 [==============================] - 25s 29ms/step - loss: 0.4561 - accuracy: 0.7836 - val_loss: 0.3882 - val_accuracy: 0.8400 - lr: 0.0010\n",
            "Epoch 3/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.4201 - accuracy: 0.8104 - val_loss: 0.4096 - val_accuracy: 0.8180 - lr: 0.0010\n",
            "Epoch 4/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.3859 - accuracy: 0.8320 - val_loss: 0.5028 - val_accuracy: 0.7940 - lr: 0.0010\n",
            "Epoch 5/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.3603 - accuracy: 0.8455 - val_loss: 0.3977 - val_accuracy: 0.8220 - lr: 0.0010\n",
            "Epoch 6/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.3163 - accuracy: 0.8639 - val_loss: 0.4056 - val_accuracy: 0.8260 - lr: 0.0010\n",
            "Epoch 7/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.2835 - accuracy: 0.8818 - val_loss: 0.4082 - val_accuracy: 0.8360 - lr: 0.0010\n",
            "Epoch 8/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.2527 - accuracy: 0.8971 - val_loss: 0.4093 - val_accuracy: 0.8260 - lr: 0.0010\n",
            "Epoch 9/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.2154 - accuracy: 0.9118 - val_loss: 0.5153 - val_accuracy: 0.8040 - lr: 0.0010\n",
            "Epoch 10/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.1918 - accuracy: 0.9273 - val_loss: 0.6333 - val_accuracy: 0.8180 - lr: 0.0010\n",
            "Epoch 11/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.1684 - accuracy: 0.9365 - val_loss: 0.5678 - val_accuracy: 0.8100 - lr: 9.0000e-04\n",
            "Epoch 12/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.1406 - accuracy: 0.9469 - val_loss: 0.5723 - val_accuracy: 0.8020 - lr: 9.0000e-04\n",
            "Epoch 13/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.1236 - accuracy: 0.9549 - val_loss: 0.6737 - val_accuracy: 0.7800 - lr: 9.0000e-04\n",
            "Epoch 14/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.1237 - accuracy: 0.9535 - val_loss: 0.7283 - val_accuracy: 0.8120 - lr: 9.0000e-04\n",
            "Epoch 15/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0881 - accuracy: 0.9685 - val_loss: 1.0058 - val_accuracy: 0.7420 - lr: 9.0000e-04\n",
            "Epoch 16/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0950 - accuracy: 0.9648 - val_loss: 0.8069 - val_accuracy: 0.7880 - lr: 9.0000e-04\n",
            "Epoch 17/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0924 - accuracy: 0.9673 - val_loss: 0.7709 - val_accuracy: 0.8080 - lr: 9.0000e-04\n",
            "Epoch 18/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0812 - accuracy: 0.9724 - val_loss: 0.6474 - val_accuracy: 0.8420 - lr: 9.0000e-04\n",
            "Epoch 19/400\n",
            "846/846 [==============================] - 30s 35ms/step - loss: 0.0722 - accuracy: 0.9737 - val_loss: 0.6066 - val_accuracy: 0.8440 - lr: 9.0000e-04\n",
            "Epoch 20/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0785 - accuracy: 0.9719 - val_loss: 0.8102 - val_accuracy: 0.8040 - lr: 9.0000e-04\n",
            "Epoch 21/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0602 - accuracy: 0.9784 - val_loss: 0.7928 - val_accuracy: 0.8260 - lr: 9.0000e-04\n",
            "Epoch 22/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0680 - accuracy: 0.9751 - val_loss: 0.8125 - val_accuracy: 0.8140 - lr: 9.0000e-04\n",
            "Epoch 23/400\n",
            "846/846 [==============================] - 22s 27ms/step - loss: 0.0569 - accuracy: 0.9795 - val_loss: 1.1780 - val_accuracy: 0.8260 - lr: 9.0000e-04\n",
            "Epoch 24/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0597 - accuracy: 0.9787 - val_loss: 0.8796 - val_accuracy: 0.7980 - lr: 9.0000e-04\n",
            "Epoch 25/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0494 - accuracy: 0.9831 - val_loss: 0.8006 - val_accuracy: 0.8340 - lr: 9.0000e-04\n",
            "Epoch 26/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0593 - accuracy: 0.9803 - val_loss: 0.7986 - val_accuracy: 0.8280 - lr: 9.0000e-04\n",
            "Epoch 27/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0476 - accuracy: 0.9831 - val_loss: 0.9002 - val_accuracy: 0.7960 - lr: 9.0000e-04\n",
            "Epoch 28/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0404 - accuracy: 0.9865 - val_loss: 0.8075 - val_accuracy: 0.8140 - lr: 8.1000e-04\n",
            "Epoch 29/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0423 - accuracy: 0.9858 - val_loss: 0.7809 - val_accuracy: 0.8280 - lr: 8.1000e-04\n",
            "Epoch 30/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0447 - accuracy: 0.9851 - val_loss: 0.8976 - val_accuracy: 0.8100 - lr: 8.1000e-04\n",
            "Epoch 31/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0421 - accuracy: 0.9860 - val_loss: 0.9445 - val_accuracy: 0.7780 - lr: 8.1000e-04\n",
            "Epoch 32/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.9282 - val_accuracy: 0.8160 - lr: 8.1000e-04\n",
            "Epoch 33/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0375 - accuracy: 0.9866 - val_loss: 0.8417 - val_accuracy: 0.8160 - lr: 8.1000e-04\n",
            "Epoch 34/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0314 - accuracy: 0.9895 - val_loss: 0.8948 - val_accuracy: 0.8380 - lr: 8.1000e-04\n",
            "Epoch 35/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0320 - accuracy: 0.9897 - val_loss: 0.8789 - val_accuracy: 0.8240 - lr: 8.1000e-04\n",
            "Epoch 36/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0308 - accuracy: 0.9887 - val_loss: 1.0825 - val_accuracy: 0.8400 - lr: 7.2900e-04\n",
            "Epoch 37/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0263 - accuracy: 0.9916 - val_loss: 0.7706 - val_accuracy: 0.8420 - lr: 7.2900e-04\n",
            "Epoch 38/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.8789 - val_accuracy: 0.8160 - lr: 7.2900e-04\n",
            "Epoch 39/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0256 - accuracy: 0.9910 - val_loss: 1.2364 - val_accuracy: 0.8400 - lr: 7.2900e-04\n",
            "Epoch 40/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 1.0301 - val_accuracy: 0.8460 - lr: 7.2900e-04\n",
            "Epoch 41/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0184 - accuracy: 0.9934 - val_loss: 1.0228 - val_accuracy: 0.8380 - lr: 7.2900e-04\n",
            "Epoch 42/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0291 - accuracy: 0.9902 - val_loss: 0.8801 - val_accuracy: 0.8300 - lr: 7.2900e-04\n",
            "Epoch 43/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0257 - accuracy: 0.9912 - val_loss: 1.2367 - val_accuracy: 0.8040 - lr: 7.2900e-04\n",
            "Epoch 44/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.8868 - val_accuracy: 0.8120 - lr: 7.2900e-04\n",
            "Epoch 45/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0240 - accuracy: 0.9922 - val_loss: 0.9768 - val_accuracy: 0.8220 - lr: 7.2900e-04\n",
            "Epoch 46/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 0.9756 - val_accuracy: 0.8200 - lr: 7.2900e-04\n",
            "Epoch 47/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0258 - accuracy: 0.9921 - val_loss: 1.2105 - val_accuracy: 0.7980 - lr: 7.2900e-04\n",
            "Epoch 48/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0200 - accuracy: 0.9934 - val_loss: 0.9695 - val_accuracy: 0.8360 - lr: 7.2900e-04\n",
            "Epoch 49/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0210 - accuracy: 0.9919 - val_loss: 0.8784 - val_accuracy: 0.8380 - lr: 6.5610e-04\n",
            "Epoch 50/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0176 - accuracy: 0.9936 - val_loss: 0.9530 - val_accuracy: 0.8280 - lr: 6.5610e-04\n",
            "Epoch 51/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0217 - accuracy: 0.9919 - val_loss: 1.0317 - val_accuracy: 0.8220 - lr: 6.5610e-04\n",
            "Epoch 52/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 1.0797 - val_accuracy: 0.8380 - lr: 6.5610e-04\n",
            "Epoch 53/400\n",
            "846/846 [==============================] - 22s 25ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 1.0299 - val_accuracy: 0.8420 - lr: 6.5610e-04\n",
            "Epoch 54/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0175 - accuracy: 0.9948 - val_loss: 1.0731 - val_accuracy: 0.8200 - lr: 6.5610e-04\n",
            "Epoch 55/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.9892 - val_accuracy: 0.8280 - lr: 6.5610e-04\n",
            "Epoch 56/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0200 - accuracy: 0.9942 - val_loss: 1.0124 - val_accuracy: 0.8160 - lr: 6.5610e-04\n",
            "Epoch 57/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0114 - accuracy: 0.9957 - val_loss: 1.0639 - val_accuracy: 0.8100 - lr: 5.9049e-04\n",
            "Epoch 58/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.8827 - val_accuracy: 0.8480 - lr: 5.9049e-04\n",
            "Epoch 59/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 1.0136 - val_accuracy: 0.8280 - lr: 5.9049e-04\n",
            "Epoch 60/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 1.5485 - val_accuracy: 0.8100 - lr: 5.9049e-04\n",
            "Epoch 61/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0166 - accuracy: 0.9949 - val_loss: 1.0108 - val_accuracy: 0.8160 - lr: 5.9049e-04\n",
            "Epoch 62/400\n",
            "846/846 [==============================] - 21s 25ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 1.0343 - val_accuracy: 0.8160 - lr: 5.9049e-04\n",
            "Epoch 63/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 1.0257 - val_accuracy: 0.8160 - lr: 5.9049e-04\n",
            "Epoch 64/400\n",
            "846/846 [==============================] - 22s 27ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 1.1631 - val_accuracy: 0.8460 - lr: 5.9049e-04\n",
            "Epoch 65/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0106 - accuracy: 0.9964 - val_loss: 1.1579 - val_accuracy: 0.8040 - lr: 5.9049e-04\n",
            "Epoch 66/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0164 - accuracy: 0.9946 - val_loss: 1.0627 - val_accuracy: 0.8240 - lr: 5.9049e-04\n",
            "Epoch 67/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 1.0550 - val_accuracy: 0.8380 - lr: 5.3144e-04\n",
            "Epoch 68/400\n",
            "846/846 [==============================] - 22s 27ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 1.0120 - val_accuracy: 0.8320 - lr: 5.3144e-04\n",
            "Epoch 69/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.9957 - val_accuracy: 0.8380 - lr: 5.3144e-04\n",
            "Epoch 70/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 1.0549 - val_accuracy: 0.8100 - lr: 5.3144e-04\n",
            "Epoch 71/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 1.2659 - val_accuracy: 0.8300 - lr: 5.3144e-04\n",
            "Epoch 72/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 1.2261 - val_accuracy: 0.8480 - lr: 5.3144e-04\n",
            "Epoch 73/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 1.0930 - val_accuracy: 0.8160 - lr: 5.3144e-04\n",
            "Epoch 74/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 1.1130 - val_accuracy: 0.8400 - lr: 5.3144e-04\n",
            "Epoch 75/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 1.2115 - val_accuracy: 0.8320 - lr: 4.7830e-04\n",
            "Epoch 76/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0203 - accuracy: 0.9950 - val_loss: 1.0694 - val_accuracy: 0.8240 - lr: 4.7830e-04\n",
            "Epoch 77/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 1.1105 - val_accuracy: 0.8360 - lr: 4.7830e-04\n",
            "Epoch 78/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 1.1216 - val_accuracy: 0.8260 - lr: 4.7830e-04\n",
            "Epoch 79/400\n",
            "846/846 [==============================] - 22s 27ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 1.0864 - val_accuracy: 0.8400 - lr: 4.7830e-04\n",
            "Epoch 80/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 1.2448 - val_accuracy: 0.8300 - lr: 4.7830e-04\n",
            "Epoch 81/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 1.2638 - val_accuracy: 0.8100 - lr: 4.7830e-04\n",
            "Epoch 82/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 1.2451 - val_accuracy: 0.8180 - lr: 4.7830e-04\n",
            "Epoch 83/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 1.0712 - val_accuracy: 0.8220 - lr: 4.3047e-04\n",
            "Epoch 84/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 1.1346 - val_accuracy: 0.8140 - lr: 4.3047e-04\n",
            "Epoch 85/400\n",
            "846/846 [==============================] - 23s 27ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 1.2956 - val_accuracy: 0.8220 - lr: 4.3047e-04\n",
            "Epoch 86/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 1.1075 - val_accuracy: 0.8240 - lr: 4.3047e-04\n",
            "Epoch 87/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.1890 - val_accuracy: 0.8360 - lr: 4.3047e-04\n",
            "Epoch 88/400\n",
            "846/846 [==============================] - 22s 26ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 1.0535 - val_accuracy: 0.8380 - lr: 4.3047e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_accuracy = ft_model.evaluate(tfk.applications.xception.preprocess_input(X_test),y_test,verbose=0)[-1]\n",
        "print('Test set accuracy %.4f' % test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erGzo711x0zD",
        "outputId": "6bbb36ec-4b7d-4e6c-af87-ac8a6297e2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy 0.8120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.save('Xception_NO_INVERSION_finetuned')\n",
        "del ft_model"
      ],
      "metadata": {
        "id": "uWbGf8SS7Mhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "izIK2ILF7V00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}